# AUTOGENERATED FILE. Only edit for testing purposes, not for development. Generated from utils/unified_config.yaml. Generated by utils/split_config.py
Panel Config:
  id: 600
  title: Workgroup Manager (SPI)
  metrics_description:
    Accelerator Utilization: The percent of cycles in the kernel where the accelerator
      was actively doing any work.
    Scheduler-Pipe Utilization: The percent of total scheduler-pipe cycles in the
      kernel where the scheduler-pipes were actively doing any work.
    Workgroup Manager Utilization: The percent of cycles in the kernel where the workgroup
      manager was actively doing any work.
    Shader Engine Utilization: The percent of total shader engine cycles in the kernel
      where any CU in a shader-engine was actively doing any work, normalized over
      all shader-engines. Low values (e.g., << 100%) indicate that the accelerator
      was not fully saturated by the kernel, or a potential load-imbalance issue.
    SIMD Utilization: The percent of total SIMD cycles in the kernel where any SIMD
      on a CU was actively doing any work, summed over all CUs. Low values (less than
      100%) indicate that the accelerator was not fully saturated by the kernel, or
      a potential load-imbalance issue.
    Dispatched Workgroups: The total number of workgroups forming this kernel launch.
    Dispatched Wavefronts: The total number of wavefronts, summed over all workgroups,
      forming this kernel launch.
    VGPR Writes: The average number of cycles spent initializing VGPRs at wave creation.
    SGPR Writes: The average number of cycles spent initializing SGPRs at wave creation.
    Not-scheduled Rate (Workgroup Manager): The percent of total scheduler-pipe cycles
      in the kernel where a workgroup could not be scheduled to a CU due to a bottleneck
      within the workgroup manager rather than a lack of a CU or SIMD with sufficient
      resources.
    Not-scheduled Rate (Scheduler-Pipe): 'The percent of total scheduler-pipe cycles
      in the kernel where a workgroup could not be scheduled to a CU due to a bottleneck
      within the scheduler-pipes rather than a lack of a CU or SIMD with sufficient
      resources. '
    Scheduler-Pipe Stall Rate: The percent of total scheduler-pipe cycles in the kernel
      where a workgroup could not be scheduled to a CU due to occupancy limitations
      (like a lack of a CU or SIMD with sufficient resources).
    Scratch Stall Rate: The percent of total shader-engine cycles in the kernel where
      a workgroup could not be scheduled to a CU due to lack of private (a.k.a., scratch)
      memory slots. While this can reach up to 100%, note that the actual occupancy
      limitations on a kernel using private memory are typically quite small (for
      example, less than 1% of the total number of waves that can be scheduled to
      an accelerator).
    Insufficient SIMD Waveslots: The percent of total SIMD cycles in the kernel where
      a workgroup could not be scheduled to a SIMD due to lack of available waveslots.
    Insufficient SIMD VGPRs: The percent of total SIMD cycles in the kernel where
      a workgroup could not be scheduled to a SIMD due to lack of available VGPRs.
    Insufficient SIMD SGPRs: The percent of total SIMD cycles in the kernel where
      a workgroup could not be scheduled to a SIMD due to lack of available SGPRs.
    Insufficient CU LDS: The percent of total CU cycles in the kernel where a workgroup
      could not be scheduled to a CU due to lack of available LDS.
    Insufficient CU Barriers: The percent of total CU cycles in the kernel where a
      workgroup could not be scheduled to a CU due to lack of available barriers.
    Reached CU Workgroup Limit: The percent of total CU cycles in the kernel where
      a workgroup could not be scheduled to a CU due to limits within the workgroup
      manager. This is expected to be always be zero on CDNA2 or newer accelerators
      (and small for previous accelerators).
    Reached CU Wavefront Limit: The percent of total CU cycles in the kernel where
      a wavefront could not be scheduled to a CU due to limits within the workgroup
      manager. This is expected to be always be zero on CDNA2 or newer accelerators
      (and small for previous accelerators).
  data source:
  - metric_table:
      id: 601
      title: Workgroup manager utilizations
      header:
        metric: Metric
        avg: Avg
        min: Min
        max: Max
        unit: Unit
      metric:
        Schedule-Pipe Wave Occupancy:
          avg: AVG(SPI_CSQ_P0_OCCUPANCY + SPI_CSQ_P1_OCCUPANCY + SPI_CSQ_P2_OCCUPANCY
            + SPI_CSQ_P3_OCCUPANCY)
          min: MIN(SPI_CSQ_P0_OCCUPANCY + SPI_CSQ_P1_OCCUPANCY + SPI_CSQ_P2_OCCUPANCY
            + SPI_CSQ_P3_OCCUPANCY)
          max: MAX(SPI_CSQ_P0_OCCUPANCY + SPI_CSQ_P1_OCCUPANCY + SPI_CSQ_P2_OCCUPANCY
            + SPI_CSQ_P3_OCCUPANCY)
          unit: Wave
        Accelerator Utilization:
          avg: AVG(100 * $GRBM_GUI_ACTIVE_PER_XCD / $GRBM_COUNT_PER_XCD)
          min: MIN(100 * $GRBM_GUI_ACTIVE_PER_XCD / $GRBM_COUNT_PER_XCD)
          max: MAX(100 * $GRBM_GUI_ACTIVE_PER_XCD / $GRBM_COUNT_PER_XCD)
          unit: Pct
        Scheduler-Pipe Utilization:
          avg: AVG(100 * (SPI_CS0_BUSY + SPI_CS1_BUSY + SPI_CS2_BUSY + SPI_CS3_BUSY)
            / ($GRBM_GUI_ACTIVE_PER_XCD * $pipes_per_gpu * $se_per_gpu))
          min: MIN(100 * (SPI_CS0_BUSY + SPI_CS1_BUSY + SPI_CS2_BUSY + SPI_CS3_BUSY)
            / ($GRBM_GUI_ACTIVE_PER_XCD * $pipes_per_gpu * $se_per_gpu))
          max: MAX(100 * (SPI_CS0_BUSY + SPI_CS1_BUSY + SPI_CS2_BUSY + SPI_CS3_BUSY)
            / ($GRBM_GUI_ACTIVE_PER_XCD * $pipes_per_gpu * $se_per_gpu))
          unit: Pct
        Scheduler-Pipe Wave Utilization:
          avg: AVG(100 * (SPI_CSC_WAVE_CNT_BUSY) / ($GRBM_GUI_ACTIVE_PER_XCD * $pipes_per_gpu
            * $se_per_gpu))
          min: MIN(100 * (SPI_CSC_WAVE_CNT_BUSY) / ($GRBM_GUI_ACTIVE_PER_XCD * $pipes_per_gpu
            * $se_per_gpu))
          max: MAX(100 * (SPI_CSC_WAVE_CNT_BUSY) / ($GRBM_GUI_ACTIVE_PER_XCD * $pipes_per_gpu
            * $se_per_gpu))
          unit: Pct
        Workgroup Manager Utilization:
          avg: AVG(100 * $GRBM_SPI_BUSY_PER_XCD / $GRBM_GUI_ACTIVE_PER_XCD)
          min: MIN(100 * $GRBM_SPI_BUSY_PER_XCD / $GRBM_GUI_ACTIVE_PER_XCD)
          max: MAX(100 * $GRBM_SPI_BUSY_PER_XCD / $GRBM_GUI_ACTIVE_PER_XCD)
          unit: Pct
        Shader Engine Utilization:
          avg: AVG(100 * SQ_BUSY_CYCLES / ($GRBM_GUI_ACTIVE_PER_XCD * $se_per_gpu))
          min: MIN(100 * SQ_BUSY_CYCLES / ($GRBM_GUI_ACTIVE_PER_XCD * $se_per_gpu))
          max: MAX(100 * SQ_BUSY_CYCLES / ($GRBM_GUI_ACTIVE_PER_XCD * $se_per_gpu))
          unit: Pct
        SIMD Utilization:
          avg: AVG(100 * SQ_BUSY_CU_CYCLES / ($GRBM_GUI_ACTIVE_PER_XCD * $cu_per_gpu))
          min: MIN(100 * SQ_BUSY_CU_CYCLES / ($GRBM_GUI_ACTIVE_PER_XCD * $cu_per_gpu))
          max: MAX(100 * SQ_BUSY_CU_CYCLES / ($GRBM_GUI_ACTIVE_PER_XCD * $cu_per_gpu))
          unit: Pct
        Dispatched Workgroups:
          avg: AVG(SPI_CS0_NUM_THREADGROUPS + SPI_CS1_NUM_THREADGROUPS + SPI_CS2_NUM_THREADGROUPS
            + SPI_CS3_NUM_THREADGROUPS)
          min: MIN(SPI_CS0_NUM_THREADGROUPS + SPI_CS1_NUM_THREADGROUPS + SPI_CS2_NUM_THREADGROUPS
            + SPI_CS3_NUM_THREADGROUPS)
          max: MAX(SPI_CS0_NUM_THREADGROUPS + SPI_CS1_NUM_THREADGROUPS + SPI_CS2_NUM_THREADGROUPS
            + SPI_CS3_NUM_THREADGROUPS)
          unit: Workgroups
        Dispatched Wavefronts:
          avg: AVG(SPI_CS0_WAVE + SPI_CS1_WAVE + SPI_CS2_WAVE + SPI_CS3_WAVE)
          min: MIN(SPI_CS0_WAVE + SPI_CS1_WAVE + SPI_CS2_WAVE + SPI_CS3_WAVE)
          max: MAX(SPI_CS0_WAVE + SPI_CS1_WAVE + SPI_CS2_WAVE + SPI_CS3_WAVE)
          unit: Wavefronts
        VGPR Writes:
          avg: AVG((((SPI_VWC0_VDATA_VALID_WR + SPI_VWC1_VDATA_VALID_WR) / (SPI_CS0_WAVE
            + SPI_CS1_WAVE + SPI_CS2_WAVE + SPI_CS3_WAVE)) if ((SPI_CS0_WAVE + SPI_CS1_WAVE
            + SPI_CS2_WAVE + SPI_CS3_WAVE) != 0) else None))
          min: MIN((((SPI_VWC0_VDATA_VALID_WR + SPI_VWC1_VDATA_VALID_WR) / (SPI_CS0_WAVE
            + SPI_CS1_WAVE + SPI_CS2_WAVE + SPI_CS3_WAVE)) if ((SPI_CS0_WAVE + SPI_CS1_WAVE
            + SPI_CS2_WAVE + SPI_CS3_WAVE) != 0) else None))
          max: MAX((((SPI_VWC0_VDATA_VALID_WR + SPI_VWC1_VDATA_VALID_WR) / (SPI_CS0_WAVE
            + SPI_CS1_WAVE + SPI_CS2_WAVE + SPI_CS3_WAVE)) if ((SPI_CS0_WAVE + SPI_CS1_WAVE
            + SPI_CS2_WAVE + SPI_CS3_WAVE) != 0) else None))
          unit: Cycles/wave
        SGPR Writes:
          avg: AVG((((1 * SPI_SWC_CSC_WR) / (SPI_CS0_WAVE + SPI_CS1_WAVE + SPI_CS2_WAVE
            + SPI_CS3_WAVE)) if ((SPI_CS0_WAVE + SPI_CS1_WAVE + SPI_CS2_WAVE + SPI_CS3_WAVE)
            != 0) else None))
          min: MIN((((1 * SPI_SWC_CSC_WR) / (SPI_CS0_WAVE + SPI_CS1_WAVE + SPI_CS2_WAVE
            + SPI_CS3_WAVE)) if ((SPI_CS0_WAVE + SPI_CS1_WAVE + SPI_CS2_WAVE + SPI_CS3_WAVE)
            != 0) else None))
          max: MAX((((1 * SPI_SWC_CSC_WR) / (SPI_CS0_WAVE + SPI_CS1_WAVE + SPI_CS2_WAVE
            + SPI_CS3_WAVE)) if ((SPI_CS0_WAVE + SPI_CS1_WAVE + SPI_CS2_WAVE + SPI_CS3_WAVE)
            != 0) else None))
          unit: Cycles/wave
  - metric_table:
      id: 602
      title: Workgroup Manager - Resource Allocation
      header:
        metric: Metric
        avg: Avg
        min: Min
        max: Max
        unit: Unit
      metric:
        Not-scheduled Rate (Workgroup Manager):
          avg: AVG((100 * SPI_RA_REQ_NO_ALLOC_CSN / ($GRBM_SPI_BUSY_PER_XCD * $se_per_gpu))
            if ($GRBM_SPI_BUSY_PER_XCD != 0) else None)
          min: MIN((100 * SPI_RA_REQ_NO_ALLOC_CSN / ($GRBM_SPI_BUSY_PER_XCD * $se_per_gpu))
            if ($GRBM_SPI_BUSY_PER_XCD != 0) else None)
          max: MAX((100 * SPI_RA_REQ_NO_ALLOC_CSN / ($GRBM_SPI_BUSY_PER_XCD * $se_per_gpu))
            if ($GRBM_SPI_BUSY_PER_XCD != 0) else None)
          unit: Pct
        Not-scheduled Rate (Scheduler-Pipe):
          avg: AVG((100 * SPI_RA_REQ_NO_ALLOC / ($GRBM_SPI_BUSY_PER_XCD * $se_per_gpu))
            if ($GRBM_SPI_BUSY_PER_XCD != 0) else None)
          min: MIN((100 * SPI_RA_REQ_NO_ALLOC / ($GRBM_SPI_BUSY_PER_XCD * $se_per_gpu))
            if ($GRBM_SPI_BUSY_PER_XCD != 0) else None)
          max: MAX((100 * SPI_RA_REQ_NO_ALLOC / ($GRBM_SPI_BUSY_PER_XCD * $se_per_gpu))
            if ($GRBM_SPI_BUSY_PER_XCD != 0) else None)
          unit: Pct
        Scheduler-Pipe FIFO Full Rate:
          avg: AVG((100 * (SPI_CS0_CRAWLER_STALL + SPI_CS1_CRAWLER_STALL + SPI_CS2_CRAWLER_STALL
            + SPI_CS3_CRAWLER_STALL) / ($GRBM_SPI_BUSY_PER_XCD * $se_per_gpu)) if
            ($GRBM_SPI_BUSY_PER_XCD != 0) else None)
          min: MIN((100 * (SPI_CS0_CRAWLER_STALL + SPI_CS1_CRAWLER_STALL + SPI_CS2_CRAWLER_STALL
            + SPI_CS3_CRAWLER_STALL) / ($GRBM_SPI_BUSY_PER_XCD * $se_per_gpu)) if
            ($GRBM_SPI_BUSY_PER_XCD != 0) else None)
          max: MAX((100 * (SPI_CS0_CRAWLER_STALL + SPI_CS1_CRAWLER_STALL + SPI_CS2_CRAWLER_STALL
            + SPI_CS3_CRAWLER_STALL) / ($GRBM_SPI_BUSY_PER_XCD * $se_per_gpu)) if
            ($GRBM_SPI_BUSY_PER_XCD != 0) else None)
          unit: Pct
        Scheduler-Pipe Stall Rate:
          avg: AVG((((100 * SPI_RA_RES_STALL_CSN) / ($GRBM_SPI_BUSY_PER_XCD * $se_per_gpu))
            if ($GRBM_SPI_BUSY_PER_XCD != 0) else None))
          min: MIN((((100 * SPI_RA_RES_STALL_CSN) / ($GRBM_SPI_BUSY_PER_XCD * $se_per_gpu))
            if ($GRBM_SPI_BUSY_PER_XCD != 0) else None))
          max: MAX((((100 * SPI_RA_RES_STALL_CSN) / ($GRBM_SPI_BUSY_PER_XCD * $se_per_gpu))
            if ($GRBM_SPI_BUSY_PER_XCD != 0) else None))
          unit: Pct
        Scratch Stall Rate:
          avg: AVG((100 * SPI_RA_TMP_STALL_CSN / ($GRBM_SPI_BUSY_PER_XCD * $se_per_gpu))
            if ($GRBM_SPI_BUSY_PER_XCD != 0) else None)
          min: MIN((100 * SPI_RA_TMP_STALL_CSN / ($GRBM_SPI_BUSY_PER_XCD * $se_per_gpu))
            if ($GRBM_SPI_BUSY_PER_XCD != 0) else None)
          max: MAX((100 * SPI_RA_TMP_STALL_CSN / ($GRBM_SPI_BUSY_PER_XCD * $se_per_gpu))
            if ($GRBM_SPI_BUSY_PER_XCD != 0) else None)
          unit: Pct
        Insufficient SIMD Waveslots:
          avg: AVG(100 * SPI_RA_WAVE_SIMD_FULL_CSN / ($GRBM_GUI_ACTIVE_PER_XCD * $cu_per_gpu))
          min: MIN(100 * SPI_RA_WAVE_SIMD_FULL_CSN / ($GRBM_GUI_ACTIVE_PER_XCD * $cu_per_gpu))
          max: MAX(100 * SPI_RA_WAVE_SIMD_FULL_CSN / ($GRBM_GUI_ACTIVE_PER_XCD * $cu_per_gpu))
          unit: Pct
        Insufficient SIMD VGPRs:
          avg: AVG(100 * SPI_RA_VGPR_SIMD_FULL_CSN / ($GRBM_GUI_ACTIVE_PER_XCD * $cu_per_gpu))
          min: MIN(100 * SPI_RA_VGPR_SIMD_FULL_CSN / ($GRBM_GUI_ACTIVE_PER_XCD * $cu_per_gpu))
          max: MAX(100 * SPI_RA_VGPR_SIMD_FULL_CSN / ($GRBM_GUI_ACTIVE_PER_XCD * $cu_per_gpu))
          unit: Pct
        Insufficient SIMD SGPRs:
          avg: AVG(100 * SPI_RA_SGPR_SIMD_FULL_CSN / ($GRBM_GUI_ACTIVE_PER_XCD * $cu_per_gpu))
          min: MIN(100 * SPI_RA_SGPR_SIMD_FULL_CSN / ($GRBM_GUI_ACTIVE_PER_XCD * $cu_per_gpu))
          max: MAX(100 * SPI_RA_SGPR_SIMD_FULL_CSN / ($GRBM_GUI_ACTIVE_PER_XCD * $cu_per_gpu))
          unit: Pct
        Insufficient CU LDS:
          avg: AVG(400 * SPI_RA_LDS_CU_FULL_CSN / ($GRBM_GUI_ACTIVE_PER_XCD * $cu_per_gpu))
          min: MIN(400 * SPI_RA_LDS_CU_FULL_CSN / ($GRBM_GUI_ACTIVE_PER_XCD * $cu_per_gpu))
          max: MAX(400 * SPI_RA_LDS_CU_FULL_CSN / ($GRBM_GUI_ACTIVE_PER_XCD * $cu_per_gpu))
          unit: Pct
        Insufficient CU Barriers:
          avg: AVG(400 * SPI_RA_BAR_CU_FULL_CSN / ($GRBM_GUI_ACTIVE_PER_XCD * $cu_per_gpu))
          min: MIN(400 * SPI_RA_BAR_CU_FULL_CSN / ($GRBM_GUI_ACTIVE_PER_XCD * $cu_per_gpu))
          max: MAX(400 * SPI_RA_BAR_CU_FULL_CSN / ($GRBM_GUI_ACTIVE_PER_XCD * $cu_per_gpu))
          unit: Pct
        Reached CU Workgroup Limit:
          avg: AVG(400 * SPI_RA_TGLIM_CU_FULL_CSN / ($GRBM_GUI_ACTIVE_PER_XCD * $cu_per_gpu))
          min: MIN(400 * SPI_RA_TGLIM_CU_FULL_CSN / ($GRBM_GUI_ACTIVE_PER_XCD * $cu_per_gpu))
          max: MAX(400 * SPI_RA_TGLIM_CU_FULL_CSN / ($GRBM_GUI_ACTIVE_PER_XCD * $cu_per_gpu))
          unit: Pct
        Reached CU Wavefront Limit:
          avg: AVG(400 * SPI_RA_WVLIM_STALL_CSN / ($GRBM_GUI_ACTIVE_PER_XCD * $cu_per_gpu))
          min: MIN(400 * SPI_RA_WVLIM_STALL_CSN / ($GRBM_GUI_ACTIVE_PER_XCD * $cu_per_gpu))
          max: MAX(400 * SPI_RA_WVLIM_STALL_CSN / ($GRBM_GUI_ACTIVE_PER_XCD * $cu_per_gpu))
          unit: Pct
